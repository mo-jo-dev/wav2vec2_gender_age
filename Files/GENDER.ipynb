{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57678ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_0.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_0.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060020a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_1.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_1.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8449782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_2.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_2.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fa674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_3.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_3.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5750990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_4.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_4.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f90af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_5.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_5.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_6.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_6.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_7.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_7.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_8.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_8.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_9.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_9.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_10.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_10.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c870cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_11.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_11.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de73cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_12.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_12.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd98e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_13.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_13.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf177254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_14.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_14.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_15.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_15.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_16.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_16.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81114346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_17.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_17.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_18.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_18.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0be4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_19.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_19.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_20.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_20.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ed299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_21.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_21.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5948c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_22.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_22.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_23.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_23.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c507d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Disable GPU and force TensorFlow to run on CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('./micro-wavllm-large/train_24.npz')\n",
    "test_data = np.load('./micro-wavllm-large/test_24.npz')\n",
    "\n",
    "train_features = train_data['features']  \n",
    "test_features = test_data['features']\n",
    "\n",
    "train_labels = pd.read_csv('y_train.csv').Gender_bin\n",
    "test_labels = pd.read_csv('y_test.csv').Gender_bin\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "train_features_cnn = np.expand_dims(train_features_normalized, axis=-1)\n",
    "test_features_cnn = np.expand_dims(test_features_normalized, axis=-1)\n",
    "\n",
    "# CNN model (using 1024 feature length)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', input_shape=(train_features_cnn.shape[1], 1)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    Conv1D(256, kernel_size=5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = cnn_model.fit(\n",
    "    train_features_cnn, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_features_cnn, test_labels_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# CNN Predictions and Classification Report\n",
    "cnn_predictions = np.argmax(cnn_model.predict(test_features_cnn), axis=1)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, cnn_predictions))\n",
    "\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=10.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(train_features, train_labels_encoded)\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "\n",
    "svm_accuracy = accuracy_score(test_labels_encoded, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, svm_predictions))\n",
    "\n",
    "# ANN Model\n",
    "ann_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(train_features_normalized.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "ann_history = ann_model.fit(\n",
    "    train_features_normalized, \n",
    "    train_labels_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluate the ANN\n",
    "ann_test_loss, ann_test_accuracy = ann_model.evaluate(test_features_normalized, test_labels_encoded)\n",
    "print(f\"ANN Test Accuracy: {ann_test_accuracy:.4f}\")\n",
    "\n",
    "# ANN Predictions and Classification Report\n",
    "ann_predictions = np.argmax(ann_model.predict(test_features_normalized), axis=1)\n",
    "print(\"ANN Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, ann_predictions))\n",
    "\n",
    "# PyTorch CNN Model\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * train_features_cnn.shape[1], 512)  # Adjusted based on your data\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(train_features_cnn, dtype=torch.float32).permute(0, 2, 1)  # Channels first for PyTorch\n",
    "y_train_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_features_cnn, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize PyTorch model, loss, and optimizer\n",
    "input_channels = 1\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = CNN1DModel(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# PyTorch Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "print(f\"PyTorch CNN Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
